{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Project - Dataset of blog posts on popular blog Signalvnoise\n",
    "\n",
    "Data Source : [Signalvnoise.com](https://m.signalvnoise.com)\n",
    "<div>\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRbvM6BCIt13nyV4IlUYLBvY63XojMinnOCyQ&usqp=CAU\" width=1000/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "Web scraping is a technique used to collect data and content from the internet. An example of web scraping is \n",
    "coping and pasting a content from a website into Excel spreadsheet, but on a very small scale.\n",
    "\n",
    "Web scraping applications also referred to as web scrapers are programmed to visit websites, grab the relevant \n",
    "pages and extract useful information. By automating this process, the bots can extract huge amount of data in a \n",
    "very short time.\n",
    "\n",
    "> ### Basic Web Scraping Principles:\n",
    "\n",
    "- Making an HTTP request to a server\n",
    "- Extracting and parsing the website's code\n",
    "- Saving the relevant data locally\n",
    "\n",
    "> ### Beautiful Soup\n",
    "\n",
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files. It makes it easy to scrape information from web pages. It provides Pythonic idioms for iterating, searching and modifying the parse tree.\n",
    "\n",
    "Beautiful Soup library helps with isolating titles and links from web pages. It can extract all the text from HTML tags and alter the HTML document with which we're working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Signalvnoise\n",
    "<div>\n",
    "<img src=\"https://archive.signalvnoise.com/assets/archive-v3.png\" width=500>\n",
    "</div>\n",
    "\n",
    "Signal v. Noise (to quote the blog directly) \"Strong opinions and shared thoughts on design, business, and tech. By the makers (and friends) of Basecamp\".\n",
    "One interesting thing about the blog is that most of the posts can be read in 5 minutes - they are concise, straight to the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Idea\n",
    "\n",
    "In this project, we will parse the signalvnoise website to get information like title, author, published date, blog url, author url and author image url from **https://m.signalvnoise.com** and collate the data into a single CSV document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Project Steps\n",
    "Here is an outline of the steps we'll follow :\n",
    "\n",
    "1. Download the web page using `requests`\n",
    "2. Parse the HTML source code using `BeautifulSoup` library\n",
    "3. Build the scraper components\n",
    "4. Compile the extracted information into Python list and dictionaries\n",
    "5. Write information into a CSV file\n",
    "6. Convert the CSV file into a `Python DataFrame`\n",
    "7. Future work and references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the web page using `request`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### **What is `request`**\n",
    "\n",
    ">Requests is a Python HTTP library that allows us to send HTTP requests to servers of websites, instead of using browsers to communicate the web.\n",
    "\n",
    ">We use `pip`, a package-management system, to install and manage softwares. Since the platform we selected is **Binder**, we would have to type a line of code `!pip install` to install `requests`. You will see lots codes of `!pip` when installing other packages.\n",
    "\n",
    ">When we attempt to use some prewritten functions from a certain library, we would use the `import` statement. e.g. When we would have to type `import requests` after installation, we are able to use any function from `requests` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the requests package\n",
    "!pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79143\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://m.signalvnoise.com'\n",
    "response = requests.get(base_url, headers=header)\n",
    "if response.status_code != 200:\n",
    "    print(f'Failed to fetch webpage {url} with status code {response.status_code}')\n",
    "else:\n",
    "    page_content = response.text\n",
    "    print(len(page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Parse the HTML source code using `BeautifulSoup` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = BeautifulSoup(page_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the scraper components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The get_page function returns the downloaded webpage as a BeautifulSoup object\n",
    "def get_page(page_number=None):\n",
    "    base_url = 'https://m.signalvnoise.com'\n",
    "    if page_number is None:\n",
    "        url = base_url\n",
    "    else:\n",
    "        url = base_url + '/page/' + page_number\n",
    "    response = requests.get(url, headers=header)\n",
    "    if response.status_code != 200:\n",
    "        print(f'Failed to fetch webpage {url} with status code {response.status_code}')\n",
    "    else:\n",
    "        page_content = response.text\n",
    "        print(len(page_content))\n",
    "        doc = BeautifulSoup(page_content, 'html.parser')\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111577\n"
     ]
    }
   ],
   "source": [
    "doc = get_page('80')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the extracted information into Python list and dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_tags = doc.find_all('header', class_='entry-header grid__item grid__item--large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_tag = header_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_webpage(header_tag):\n",
    "    \n",
    "    # Get title\n",
    "    h2_tag = header_tag.find('h2', class_='entry-title entry-title--list centered')\n",
    "    title = h2_tag.text.replace(',', '')\n",
    "    # Get blog url\n",
    "    blog_url = h2_tag.find('a')['href'].replace(',', '')\n",
    "    # Get author\n",
    "    author_span_tag = header_tag.find('span', class_='byline')\n",
    "    a_tag = author_span_tag.find('a')\n",
    "    author = a_tag.text.replace(',', '')\n",
    "    # Get author url\n",
    "    author_url = a_tag['href'].replace(',', '')\n",
    "    # Get published date\n",
    "    published_date = header_tag.find('time', class_='entry-date published updated').text.replace(',', '')\n",
    "    # Get author image\n",
    "    img_div = header_tag.find('div', class_='entry-meta__avatars')\n",
    "    img_url = img_div.find('img')['src'].replace(',', '')\n",
    "    return {\n",
    "        'author name': author,\n",
    "        'title': title,\n",
    "        'published_date': published_date,\n",
    "        'blog url': blog_url,\n",
    "        'author url': author_url,\n",
    "        'author image url': img_url\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author name': 'Connor Muirhead',\n",
       " 'title': 'Transforming a screen with a few questions',\n",
       " 'published_date': 'March 28 2016',\n",
       " 'blog url': 'https://m.signalvnoise.com/transforming-a-screen-with-a-few-questions/',\n",
       " 'author url': 'https://m.signalvnoise.com/author/connor-muirhead/',\n",
       " 'author image url': 'https://secure.gravatar.com/avatar/98cad650a760775077d9a8ec4c87ed8f?s=60&d=retro&r=pg'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_webpage(header_tags[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_webpages = [parse_webpage(tag) for tag in header_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_webpage(doc):\n",
    "  header_tags = doc.find_all('header', class_='entry-header grid__item grid__item--large')\n",
    "  top_webpages = [parse_webpage(tag) for tag in header_tags]\n",
    "  return top_webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write information into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in a csv\n",
    "def write_csv(items, path):\n",
    "    # Open the file in write mode\n",
    "    with open(path, 'w') as f:\n",
    "        # Return if there's nothing to write\n",
    "        if len(items) == 0:\n",
    "            return\n",
    "        \n",
    "        # Write the headers in the first line\n",
    "        headers = list(items[0].keys())\n",
    "        f.write(','.join(headers) + '\\n')\n",
    "        \n",
    "        # Write one item per line\n",
    "        for item in items:\n",
    "            values = []\n",
    "            for header in headers:\n",
    "                values.append(str(item.get(header, \"\")))\n",
    "            f.write(','.join(values) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(top_webpages, 'zero.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a CSV file, we can use pandas library to view its contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the CSV file into a Python DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author name</th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>blog url</th>\n",
       "      <th>author url</th>\n",
       "      <th>author image url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chris Gallo</td>\n",
       "      <td>Be the Plumber</td>\n",
       "      <td>March 28 2016</td>\n",
       "      <td>https://m.signalvnoise.com/be-the-plumber/</td>\n",
       "      <td>https://m.signalvnoise.com/author/chris-gallo/</td>\n",
       "      <td>https://secure.gravatar.com/avatar/?s=60&amp;d=ret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Connor Muirhead</td>\n",
       "      <td>Transforming a screen with a few questions</td>\n",
       "      <td>March 28 2016</td>\n",
       "      <td>https://m.signalvnoise.com/transforming-a-scre...</td>\n",
       "      <td>https://m.signalvnoise.com/author/connor-muirh...</td>\n",
       "      <td>https://secure.gravatar.com/avatar/98cad650a76...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jason Fried</td>\n",
       "      <td>March 2016 Basecamp 3 updates!</td>\n",
       "      <td>March 28 2016</td>\n",
       "      <td>https://m.signalvnoise.com/march-2016-basecamp...</td>\n",
       "      <td>https://m.signalvnoise.com/author/jason-fried/</td>\n",
       "      <td>https://i0.wp.com/m.signalvnoise.com/wp-conten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jamis Buck</td>\n",
       "      <td>To Smile Again</td>\n",
       "      <td>March 28 2016</td>\n",
       "      <td>https://m.signalvnoise.com/to-smile-again/</td>\n",
       "      <td>https://m.signalvnoise.com/author/jamis-buck/</td>\n",
       "      <td>https://secure.gravatar.com/avatar/?s=60&amp;d=ret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chase Clemons</td>\n",
       "      <td>The Lost Coffee Order</td>\n",
       "      <td>March 28 2016</td>\n",
       "      <td>https://m.signalvnoise.com/the-lost-coffee-order/</td>\n",
       "      <td>https://m.signalvnoise.com/author/chase-clemons/</td>\n",
       "      <td>https://secure.gravatar.com/avatar/3b9b431e2b1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jonas Downey</td>\n",
       "      <td>You Aren’t Gonna Need to Design It</td>\n",
       "      <td>March 25 2016</td>\n",
       "      <td>https://m.signalvnoise.com/you-arent-gonna-nee...</td>\n",
       "      <td>https://m.signalvnoise.com/author/jonas-downey/</td>\n",
       "      <td>https://secure.gravatar.com/avatar/3c3dc2f9818...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jason Fried</td>\n",
       "      <td>The team the years</td>\n",
       "      <td>March 23 2016</td>\n",
       "      <td>https://m.signalvnoise.com/the-team-the-years/</td>\n",
       "      <td>https://m.signalvnoise.com/author/jason-fried/</td>\n",
       "      <td>https://i0.wp.com/m.signalvnoise.com/wp-conten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DHH</td>\n",
       "      <td>Sleep deprivation is not a badge of honor</td>\n",
       "      <td>March 23 2016</td>\n",
       "      <td>https://m.signalvnoise.com/sleep-deprivation-i...</td>\n",
       "      <td>https://m.signalvnoise.com/author/dhh/</td>\n",
       "      <td>https://secure.gravatar.com/avatar/040ac7f6cb7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DHH</td>\n",
       "      <td>Simple just isn’t that important</td>\n",
       "      <td>March 21 2016</td>\n",
       "      <td>https://m.signalvnoise.com/simple-just-isnt-th...</td>\n",
       "      <td>https://m.signalvnoise.com/author/dhh/</td>\n",
       "      <td>https://secure.gravatar.com/avatar/040ac7f6cb7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chase Clemons</td>\n",
       "      <td>No Reply Addresses</td>\n",
       "      <td>March 18 2016</td>\n",
       "      <td>https://m.signalvnoise.com/no-reply-addresses/</td>\n",
       "      <td>https://m.signalvnoise.com/author/chase-clemons/</td>\n",
       "      <td>https://secure.gravatar.com/avatar/3b9b431e2b1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author name                                       title published_date  \\\n",
       "0      Chris Gallo                              Be the Plumber  March 28 2016   \n",
       "1  Connor Muirhead  Transforming a screen with a few questions  March 28 2016   \n",
       "2      Jason Fried              March 2016 Basecamp 3 updates!  March 28 2016   \n",
       "3       Jamis Buck                              To Smile Again  March 28 2016   \n",
       "4    Chase Clemons                       The Lost Coffee Order  March 28 2016   \n",
       "5     Jonas Downey          You Aren’t Gonna Need to Design It  March 25 2016   \n",
       "6      Jason Fried                          The team the years  March 23 2016   \n",
       "7              DHH   Sleep deprivation is not a badge of honor  March 23 2016   \n",
       "8              DHH            Simple just isn’t that important  March 21 2016   \n",
       "9    Chase Clemons                          No Reply Addresses  March 18 2016   \n",
       "\n",
       "                                            blog url  \\\n",
       "0         https://m.signalvnoise.com/be-the-plumber/   \n",
       "1  https://m.signalvnoise.com/transforming-a-scre...   \n",
       "2  https://m.signalvnoise.com/march-2016-basecamp...   \n",
       "3         https://m.signalvnoise.com/to-smile-again/   \n",
       "4  https://m.signalvnoise.com/the-lost-coffee-order/   \n",
       "5  https://m.signalvnoise.com/you-arent-gonna-nee...   \n",
       "6     https://m.signalvnoise.com/the-team-the-years/   \n",
       "7  https://m.signalvnoise.com/sleep-deprivation-i...   \n",
       "8  https://m.signalvnoise.com/simple-just-isnt-th...   \n",
       "9     https://m.signalvnoise.com/no-reply-addresses/   \n",
       "\n",
       "                                          author url  \\\n",
       "0     https://m.signalvnoise.com/author/chris-gallo/   \n",
       "1  https://m.signalvnoise.com/author/connor-muirh...   \n",
       "2     https://m.signalvnoise.com/author/jason-fried/   \n",
       "3      https://m.signalvnoise.com/author/jamis-buck/   \n",
       "4   https://m.signalvnoise.com/author/chase-clemons/   \n",
       "5    https://m.signalvnoise.com/author/jonas-downey/   \n",
       "6     https://m.signalvnoise.com/author/jason-fried/   \n",
       "7             https://m.signalvnoise.com/author/dhh/   \n",
       "8             https://m.signalvnoise.com/author/dhh/   \n",
       "9   https://m.signalvnoise.com/author/chase-clemons/   \n",
       "\n",
       "                                    author image url  \n",
       "0  https://secure.gravatar.com/avatar/?s=60&d=ret...  \n",
       "1  https://secure.gravatar.com/avatar/98cad650a76...  \n",
       "2  https://i0.wp.com/m.signalvnoise.com/wp-conten...  \n",
       "3  https://secure.gravatar.com/avatar/?s=60&d=ret...  \n",
       "4  https://secure.gravatar.com/avatar/3b9b431e2b1...  \n",
       "5  https://secure.gravatar.com/avatar/3c3dc2f9818...  \n",
       "6  https://i0.wp.com/m.signalvnoise.com/wp-conten...  \n",
       "7  https://secure.gravatar.com/avatar/040ac7f6cb7...  \n",
       "8  https://secure.gravatar.com/avatar/040ac7f6cb7...  \n",
       "9  https://secure.gravatar.com/avatar/3b9b431e2b1...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('zero.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "base_url = 'https://m.signalvnoise.com'\n",
    "\n",
    "def scrape_page(page_number, path=None):\n",
    "    # Extract information from a page and write them to a CSV file\"\n",
    "    if path is None:\n",
    "        path = page_number + '.csv'\n",
    "    doc2 = get_page(page_number)\n",
    "    top_webpages1 = top_webpage(doc2)\n",
    "    write_csv(top_webpages1, path)\n",
    "    print('Extracted information for page {} written to file {}'.format(page_number, path))\n",
    "    return path\n",
    "\n",
    "def parse_webpage(header_tag):\n",
    "    \n",
    "    # Get title\n",
    "    h2_tag = header_tag.find('h2', class_='entry-title entry-title--list centered')\n",
    "    title = h2_tag.text.replace(',', '')\n",
    "    # Get blog url\n",
    "    blog_url = h2_tag.find('a')['href'].replace(',', '')\n",
    "    # Get author\n",
    "    author_span_tag = header_tag.find('span', class_='byline')\n",
    "    a_tag = author_span_tag.find('a')\n",
    "    author = a_tag.text.replace(',', '')\n",
    "    # Get author url\n",
    "    author_url = a_tag['href'].replace(',', '')\n",
    "    # Get published date\n",
    "    published_date = header_tag.find('time', class_='entry-date published updated').text.replace(',', '')\n",
    "    # Get author image\n",
    "    img_div = header_tag.find('div', class_='entry-meta__avatars')\n",
    "    img_url = img_div.find('img')['src'].replace(',', '')\n",
    "    return {\n",
    "        'author name': author,\n",
    "        'title': title,\n",
    "        'published_date': published_date,\n",
    "        'blog url': blog_url,\n",
    "        'author url': author_url,\n",
    "        'author image url': img_url\n",
    "    }\n",
    "\n",
    "# The get_page function returns the downloaded webpage as a BeautifulSoup object\n",
    "def get_page(page_number=None):\n",
    "    base_url = 'https://m.signalvnoise.com'\n",
    "    if page_number is None:\n",
    "        url = base_url\n",
    "    else:\n",
    "        url = base_url + '/page/' + page_number\n",
    "    response = requests.get(url, headers=header)\n",
    "    if response.status_code != 200:\n",
    "        print(f'Failed to fetch webpage {url} with status code {response.status_code}')\n",
    "    else:\n",
    "        page_content = response.text\n",
    "        print(len(page_content))\n",
    "        doc = BeautifulSoup(page_content, 'html.parser')\n",
    "        return doc\n",
    "    \n",
    "# Put in a csv\n",
    "def write_csv(items, path):\n",
    "    # Open the file in write mode\n",
    "    with open(path, 'w') as f:\n",
    "        # Return if there's nothing to write\n",
    "        if len(items) == 0:\n",
    "            return\n",
    "        \n",
    "        # Write the headers in the first line\n",
    "        headers = list(items[0].keys())\n",
    "        f.write(','.join(headers) + '\\n')\n",
    "        \n",
    "        # Write one item per line\n",
    "        for item in items:\n",
    "            values = []\n",
    "            for header in headers:\n",
    "                values.append(str(item.get(header, \"\")))\n",
    "            f.write(','.join(values) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82015\n",
      "Extracted information for page 2 written to file 2.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.csv'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_page('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"menkachi85/scrape-signalnvoice\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/menkachi85/scrape-signalnvoice\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/menkachi85/scrape-signalnvoice'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " jovian.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
